{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distinguishing Cited and Repackaged Instances, Topic Modeling, Mapping, & Link Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distinguishing Cited and Repackaged Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # used for creating/manipulating data frames\n",
    "\n",
    "# read in data\n",
    "foundlinks = pd.read_excel(\"foundlinks.modeling 1.xlsx\")\n",
    "foundlinks.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cleaning again to the data just to be safe\n",
    "\n",
    "#getting rid of special characters and lower caseing content\n",
    "pattern = r'[^a-zA-Z0-9\\s]'\n",
    "foundlinks['content'] = foundlinks['content'].str.replace(pattern,'',regex=True)\n",
    "# Remove new line characters\n",
    "foundlinks['content'] = foundlinks['content'].str.replace(r'\\n', '', regex=True)\n",
    "foundlinks = foundlinks.applymap(lambda x: x.lower() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sites had multiple found links, combining them to reduce duplicate URls and content\n",
    "unique_links_per_url = foundlinks.groupby('url')['Found Links'].unique().reset_index()\n",
    "unique_links_per_url # view data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#key words for finding if site is repackaged or cited\n",
    "\n",
    "keywords = [\"tables\",\"table\",\"data\",\"quickfacts\"]\n",
    "\n",
    "pattern = '|'.join(keywords)  \n",
    "\n",
    "# Function to check if any link in the list contains a keyword\n",
    "def contains_keyword(links, pattern):\n",
    "    # Return True if any link contains a keyword, otherwise False\n",
    "    return any(pd.notna(link) and pd.Series(link).str.contains(pattern, case=False, regex=True).any() \n",
    "               for link in links)\n",
    "\n",
    "# Apply the function to create a new column 'contains_keywords'\n",
    "unique_links_per_url['Source'] = unique_links_per_url['Found Links'].apply(lambda x: contains_keyword(x, pattern))\n",
    "unique_links_per_url['Source'] = unique_links_per_url['Source'].replace({True: 'repackaged', False: 'cited'})\n",
    "\n",
    "unique_links_per_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining content since we combined the links\n",
    "combined_content = foundlinks.groupby('url')['content'].apply(' '.join).reset_index()\n",
    "combined_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and view final data\n",
    "final_data = pd.merge(unique_links_per_url, combined_content, on='url')\n",
    "final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer # turn corpus of text into document-term matrix\n",
    "from gensim.corpora import Dictionary # create dictionary\n",
    "from nltk.corpus import stopwords # removal of stopwords\n",
    "from nltk.stem import WordNetLemmatizer # lemmatize text\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define stop words\n",
    "stop_words = stopwords.words('english') # standard English stopwords\n",
    "stop_words.extend([\"data\", \"census\", \"bureau\"]) # words specific to this data set that add little meaning\n",
    "\n",
    "# Preprocessing function to lemmatize text\n",
    "def preprocess(text):\n",
    "    tokens = text.split()  # tokenization\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]  \n",
    "    return ' '.join(lemmatized)  # Return the lemmatized text\n",
    "\n",
    "# Apply the preprocessing function to the content\n",
    "final_data['content'] = final_data['content'].apply(preprocess)\n",
    "\n",
    "# Vectorize the content with CountVectorizer\n",
    "vec = CountVectorizer(stop_words=stop_words, max_df=0.8, min_df=0.2, token_pattern=r'\\b[a-zA-Z]{3,}\\b') \n",
    "X = vec.fit_transform(final_data['content']) # removes stopwords, words in more than 80% and less than 20% of records, numbers, and words with less than 3 characters\n",
    "\n",
    "# Map terms to their ids for gensim compatibility\n",
    "terms = vec.get_feature_names_out()\n",
    "\n",
    "# Prepare the texts as tokenized lists (for coherence calculation)\n",
    "tokenized_texts = [doc.split() for doc in final_data['content']]\n",
    "\n",
    "# Create dictionary for coherence model\n",
    "dictionary = Dictionary(tokenized_texts)\n",
    "\n",
    "# Range of topic numbers to test\n",
    "topics_range = range(3, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # numpy functions\n",
    "from sklearn.decomposition import LatentDirichletAllocation # LDA model\n",
    "from gensim.models.coherencemodel import CoherenceModel # coherence scores\n",
    "from sklearn.preprocessing import normalize # exclusivity calc\n",
    "\n",
    "# Define a function to calculate coherence and exclusivity\n",
    "def evaluate_topic_model(X, tokenized_texts, terms, num_topics, dictionary):\n",
    "    # Initialize LDA\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=225)\n",
    "    lda.fit(X)\n",
    "\n",
    "    # Calculate Coherence\n",
    "    topics = []\n",
    "    for topic in lda.components_:\n",
    "        top_word_ids = topic.argsort()[-20:][::-1]  # analyzes top 20 words\n",
    "        topic_words = [terms[i] for i in top_word_ids]\n",
    "        topics.append(topic_words)\n",
    "\n",
    "    coherence_model = CoherenceModel(\n",
    "        topics=topics,\n",
    "        texts=tokenized_texts,\n",
    "        dictionary=dictionary,\n",
    "        coherence='u_mass'\n",
    "    )\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "    # Calculate Exclusivity\n",
    "    topic_word_distributions = normalize(lda.components_, norm='l1', axis=1)\n",
    "    exclusivity_scores = []\n",
    "    for i, topic in enumerate(topic_word_distributions):\n",
    "        top_word_ids = topic.argsort()[-20:][::-1]\n",
    "        top_words = [terms[idx] for idx in top_word_ids]\n",
    "\n",
    "        # Calculate exclusivity as uniqueness of each word in the topic\n",
    "        exclusivity = 0\n",
    "        for word in top_words:\n",
    "            word_count = sum(word in other_topic for other_topic in topics)\n",
    "            exclusivity += 1 / word_count  # Higher exclusivity if the word is rare across topics\n",
    "        exclusivity_scores.append(exclusivity / len(top_words))  # Average exclusivity for this topic\n",
    "\n",
    "    # Average exclusivity across all topics\n",
    "    average_exclusivity = np.mean(exclusivity_scores)\n",
    "\n",
    "    return coherence_score, average_exclusivity\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for num_topics in topics_range:\n",
    "    coherence_score, exclusivity_score = evaluate_topic_model(X, tokenized_texts, terms, num_topics, dictionary)\n",
    "    results.append({\n",
    "        'num_topics': num_topics,\n",
    "        'coherence': coherence_score,\n",
    "        'exclusivity': exclusivity_score\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "for result in results:\n",
    "    print(f\"Topics: {result['num_topics']}, Coherence: {result['coherence']:.4f}, Exclusivity: {result['exclusivity']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px # plotting\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Line plot for coherence over the range of topics\n",
    "fig = px.line(\n",
    "    df_results,\n",
    "    x='num_topics',\n",
    "    y=['coherence',],\n",
    "    labels={'num_topics': 'Number of Topics', 'value': 'Score'},\n",
    "    title='Coherence Scores for Different Number of Topics'\n",
    ")\n",
    "\n",
    "# Update layout for better readability\n",
    "fig.update_layout(\n",
    "    xaxis_title='Number of Topics',\n",
    "    yaxis_title='Score',\n",
    "    legend_title_text='Metrics',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plot for exclusivity over the range of topics\n",
    "fig = px.line(\n",
    "    df_results,\n",
    "    x='num_topics',\n",
    "    y=['exclusivity'],\n",
    "    labels={'num_topics': 'Number of Topics', 'value': 'Score'},\n",
    "    title='Exclusivity Scores for Different Number of Topics'\n",
    ")\n",
    "\n",
    "# Update layout for better readability\n",
    "fig.update_layout(\n",
    "    xaxis_title='Number of Topics',\n",
    "    yaxis_title='Score',\n",
    "    legend_title_text='Metrics',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interactive graph of coherance and exclusivity\n",
    "fig = px.scatter(\n",
    "    df_results,\n",
    "    x='coherence', \n",
    "    y='exclusivity',\n",
    "    color='num_topics', \n",
    "    labels={\n",
    "        'coherence': 'Coherence Score',\n",
    "        'exclusivity': 'Exclusivity Score',\n",
    "        'num_topics': 'Number of Topics'\n",
    "    },\n",
    "    title='Coherence vs. Exclusivity Scores for Different Number of Topics'\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=10), hovertemplate='Num Topics: %{marker.color}<br>Coherence: %{x}<br>Exclusivity: %{y}')\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Coherence Score',\n",
    "    yaxis_title='Exclusivity Score',\n",
    "    coloraxis_colorbar=dict(title='Number of Topics')\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA Topic Modeling\n",
    "vec = CountVectorizer(stop_words=stop_words, max_df=0.80, min_df=0.20, token_pattern=r'\\b[a-zA-Z]{3,}\\b') # vectorizer\n",
    "X = vec.fit_transform(final_data['content']) # transform content \n",
    "lda = LatentDirichletAllocation(n_components=4, random_state=225) # LDA modeling\n",
    "doc_topics = lda.fit_transform(X)\n",
    "print(lda.components_.shape[0]) #topics\n",
    "print(lda.components_.shape[1]) #words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(vec.get_feature_names_out()) # extract words\n",
    "\n",
    "n_words = 20 # top 20 words\n",
    "n_topics = 4 # 4 topics\n",
    "\n",
    "def imp_words(topic, n_words):\n",
    "    # Get indices of the top 'n_words' in descending order\n",
    "    top_word_indices = np.argsort(topic)[-n_words:][::-1]\n",
    "    return vocab[top_word_indices]\n",
    "words_in_topic = [imp_words(topic, n_words) for topic in lda.components_]\n",
    "\n",
    "# Display only the specified number of topics\n",
    "for idx, words in enumerate(words_in_topic[:n_topics]):\n",
    "    print(f\"Topic #{idx}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize topic-word distributions to get probabilities\n",
    "topic_word_distributions = normalize(lda.components_, norm='l1', axis=1)\n",
    "\n",
    "# Collect top words and their probabilities for each topic\n",
    "top_words_with_probs = {}\n",
    "for topic_idx, topic in enumerate(topic_word_distributions):\n",
    "    top_word_ids = topic.argsort()[-n_words:][::-1]  # Get indices of top n words for this topic\n",
    "    top_words_probs = [(vocab[i], topic[i]) for i in top_word_ids]  # Get word-probability pairs\n",
    "    top_words_with_probs[f'Topic {topic_idx}'] = top_words_probs\n",
    "\n",
    "# Display the results\n",
    "for topic, words in top_words_with_probs.items():\n",
    "    print(f\"{topic}:\")\n",
    "    for word, prob in words:\n",
    "        print(f\"   {word}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting topics\n",
    "plot_data = []\n",
    "for topic, words in top_words_with_probs.items():\n",
    "    for word, prob in words:\n",
    "        plot_data.append({'Topic': topic, 'Word': word, 'Probability': prob})\n",
    "\n",
    "# break up data into each topic\n",
    "df_plot = pd.DataFrame(plot_data)\n",
    "df_plot_0 = df_plot[df_plot[\"Topic\"]==\"Topic 0\"]\n",
    "df_plot_1 = df_plot[df_plot[\"Topic\"]==\"Topic 1\"]\n",
    "df_plot_2 = df_plot[df_plot[\"Topic\"]==\"Topic 2\"]\n",
    "df_plot_3 = df_plot[df_plot[\"Topic\"]==\"Topic 3\"]\n",
    "df_plot_4 = df_plot[df_plot[\"Topic\"]==\"Topic 4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # plotting\n",
    "import seaborn as sns # plotting\n",
    "\n",
    "# Plotting the distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df_plot_0, x='Word', y='Probability', palette='viridis')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Top Words Distribution - Topic 0')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Probability')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df_plot_1, x='Word', y='Probability', palette='viridis')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Top Words Distribution - Topic 1')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Probability')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df_plot_2, x='Word', y='Probability', palette='viridis')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Top Words Distribution - Topic 2')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Probability')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df_plot_3, x='Word', y='Probability', palette='viridis')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Top Words Distribution - Topic 3')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Probability')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probability of each document per topic\n",
    "cols = [\"Topic_\" + str(each) for each in range(lda.n_components)]\n",
    "docs = [\"Document_\" + str(each) for each in range(X.shape[0])]\n",
    "\n",
    "df_topics = pd.DataFrame(np.round(doc_topics,2),\n",
    "                         columns=cols,\n",
    "                         index=docs)\n",
    "\n",
    "df_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning each document the topic with the highest probability\n",
    "imp_topic = np.argmax(df_topics.values, axis=1)\n",
    "df_topics[\"top_topic\"] = imp_topic\n",
    "print(df_topics.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding topics into original DF\n",
    "conditions = [df_topics[\"top_topic\"] == 0, df_topics[\"top_topic\"] == 1, df_topics[\"top_topic\"] == 2, \n",
    "              df_topics[\"top_topic\"] == 3]\n",
    "\n",
    "choices = [\"Community and Population Data\", \n",
    "           \"Health and Demographic Surveys\", \n",
    "           \"Public Records and Services\", \n",
    "           \"Business and Economic Indicators\"]\n",
    "final_data.loc[:,\"Topic_Name\"] = np.select(conditions, choices, default=\"Other\")\n",
    "\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple count of topics\n",
    "final_data['Topic_Name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into repackaged and cited instances\n",
    "final_data_repackaged = final_data[final_data['Source']=='repackaged']\n",
    "final_data_cited = final_data[final_data['Source']=='cited']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap # wrap axis label\n",
    "\n",
    "\n",
    "# Assuming final_data_repackaged and final_data_cited are already defined DataFrames\n",
    "\n",
    "# Get the value counts for repackaged topics\n",
    "repackaged_topic_counts = final_data_repackaged['Topic_Name'].value_counts()\n",
    "\n",
    "# Get the value counts for cited topics\n",
    "cited_topic_counts = final_data_cited['Topic_Name'].value_counts()\n",
    "\n",
    "# Combine the two series into a DataFrame\n",
    "combined_counts = pd.DataFrame({\n",
    "    'Repackaged': repackaged_topic_counts,\n",
    "    'Cited': cited_topic_counts\n",
    "}).fillna(0)\n",
    "\n",
    "# Define distinct colors for repackaged and cited bars\n",
    "colors = ['red', 'blue']  # Red for Repackaged, Blue for Cited\n",
    "\n",
    "# Wrap the x-axis labels\n",
    "wrapped_labels = [textwrap.fill(label, width=20) for label in combined_counts.index]\n",
    "\n",
    "# Create a bar chart with grouped bars\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "combined_counts.plot(kind='bar', color=colors, ax=ax)\n",
    "ax.set_title('Distribution of Repackaged and Cited Topics')\n",
    "ax.set_xlabel('Topics')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xticks(range(len(wrapped_labels)))\n",
    "ax.set_xticklabels(wrapped_labels, rotation=0)\n",
    "ax.grid(axis='y')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repackaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping packages\n",
    "from geopy.geocoders import Nominatim\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import socket\n",
    "import time\n",
    "from ip2geotools.databases.noncommercial import DbIpCity\n",
    "import idna\n",
    "\n",
    "# This code can sometimes not work due to completing too many api requests\n",
    "\n",
    "# Function to get the details of the IP address including city, country, coordinates, etc.\n",
    "def get_details(ip):\n",
    "    res = DbIpCity.get(ip, api_key=\"free\")\n",
    "    return {\n",
    "        \"IP Address\": res.ip_address,\n",
    "        \"City\": res.city,\n",
    "        \"Region\": res.region,\n",
    "        \"Country\": res.country,\n",
    "        \"latitude\": res.latitude, # we will get errors for latitude and longitude; this is resolved in the next step\n",
    "        \"longitude\": res.longitude\n",
    "    }\n",
    "\n",
    "# Function to process a list of URLs and save the results into a DataFrame\n",
    "def process_urls(urls):\n",
    "    data = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            domain = url.split('//')[-1].split('/')[0]\n",
    "            ip_add = socket.gethostbyname(domain)\n",
    "            details = get_details(ip_add)\n",
    "            details[\"URL\"] = url\n",
    "            data.append(details)\n",
    "            time.sleep(1)  # Add delay to prevent rate limiting\n",
    "        except socket.gaierror:\n",
    "            print(f\"Could not resolve {url}: DNS resolution failed\")\n",
    "        except idna.core.IDNAError as e:\n",
    "            print(f\"Could not process {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not process {url}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# List of URLs to process\n",
    "urls_repackaged = final_data_repackaged['url']\n",
    "urls_repackaged = list(set(urls_repackaged)) # remove duplicate urls\n",
    "\n",
    "# Process each URL and save the results into a DataFrame\n",
    "map_df_repackaged = process_urls(urls_repackaged)\n",
    "print(map_df_repackaged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df_repackaged['City'] = map_df_repackaged['City'].apply(lambda x: re.sub(r'\\(.*?\\)', '', x).strip()) # removing data in parenthesis for city so we can get lat and long\n",
    "\n",
    "# Initialize the geolocator\n",
    "geolocator = Nominatim(user_agent=\"capstone_data_pull2\")\n",
    "\n",
    "# Function to get latitude and longitude\n",
    "def get_lat_long(city, region, country):\n",
    "    location = geolocator.geocode(f\"{city}, {region}, {country}\", timeout=10)\n",
    "    if location:\n",
    "        return location.latitude, location.longitude\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "for index, row in tqdm(map_df_repackaged.iterrows(), total=map_df_repackaged.shape[0]):\n",
    "    latitude, longitude = get_lat_long(row['City'], row['Region'], row['Country'])\n",
    "    map_df_repackaged.at[index, 'latitude'] = latitude\n",
    "    map_df_repackaged.at[index, 'longitude'] = longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the column\n",
    "map_df_repackaged.rename(columns={'URL': 'url'}, inplace=True)\n",
    "\n",
    "# Merge the DataFrames on the 'URL' column\n",
    "map_df_repackaged_merged = pd.merge(map_df_repackaged, final_data_repackaged, on='url', how='inner')\n",
    "map_df_repackaged_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping data\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Define a color map for the topics\n",
    "color_map = {\n",
    "    'Community and Population Data': 'blue',\n",
    "    'Health and Demographic Surveys': 'purple',\n",
    "    'Public Records and Services': 'red',\n",
    "    'Business and Economic Indicators': 'pink',\n",
    "    'Other': 'gray'\n",
    "}\n",
    "\n",
    "# Map the Topic_Name to colors\n",
    "map_df_repackaged_merged['color'] = map_df_repackaged_merged['Topic_Name'].map(color_map)\n",
    "\n",
    "# Create the scatter geo plot\n",
    "fig = go.Figure(data=go.Scattergeo(\n",
    "    lon=map_df_repackaged_merged['longitude'],\n",
    "    lat=map_df_repackaged_merged['latitude'],\n",
    "    mode='markers',\n",
    "    text=map_df_repackaged_merged['url'],\n",
    "    hoverinfo='text',\n",
    "    marker=dict(color=map_df_repackaged_merged['color'], size=10, line=dict(width=1, color='black')),\n",
    "    showlegend=False\n",
    "))\n",
    "\n",
    "# Update the layout to include a legend\n",
    "fig.update_layout(\n",
    "    title='Instances of Repackaged Census Data',\n",
    "    geo_scope='north america',\n",
    "    legend_title_text='Topics',\n",
    "    legend=dict(\n",
    "        itemsizing='constant'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add legend items manually\n",
    "for topic, color in color_map.items():\n",
    "    fig.add_trace(go.Scattergeo(\n",
    "        lon=[None], lat=[None],\n",
    "        mode='markers',\n",
    "        marker=dict(size=10, color=color),\n",
    "        name=topic\n",
    "    ))\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping packages\n",
    "from geopy.geocoders import Nominatim\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import socket\n",
    "import time\n",
    "from ip2geotools.databases.noncommercial import DbIpCity\n",
    "import idna\n",
    "\n",
    "# Function to get the details of the IP address including city, country, coordinates, etc.\n",
    "def get_details(ip):\n",
    "    res = DbIpCity.get(ip, api_key=\"free\")\n",
    "    return {\n",
    "        \"IP Address\": res.ip_address,\n",
    "        \"City\": res.city,\n",
    "        \"Region\": res.region,\n",
    "        \"Country\": res.country,\n",
    "        \"latitude\": res.latitude, # we will get errors for latitude and longitude; this is resolved in the next step\n",
    "        \"longitude\": res.longitude\n",
    "    }\n",
    "\n",
    "# Function to process a list of URLs and save the results into a DataFrame\n",
    "def process_urls(urls):\n",
    "    data = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            domain = url.split('//')[-1].split('/')[0]\n",
    "            ip_add = socket.gethostbyname(domain)\n",
    "            details = get_details(ip_add)\n",
    "            details[\"URL\"] = url\n",
    "            data.append(details)\n",
    "            time.sleep(1)  # Add delay to prevent rate limiting\n",
    "        except socket.gaierror:\n",
    "            print(f\"Could not resolve {url}: DNS resolution failed\")\n",
    "        except idna.core.IDNAError as e:\n",
    "            print(f\"Could not process {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not process {url}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# List of URLs to process\n",
    "urls_cited = final_data_cited['url']\n",
    "urls_cited = list(set(urls_cited)) # remove duplicate urls\n",
    "\n",
    "# Process each URL and save the results into a DataFrame\n",
    "map_df_cited = process_urls(urls_cited)\n",
    "print(map_df_cited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df_cited['City'] = map_df_cited['City'].apply(lambda x: re.sub(r'\\(.*?\\)', '', x).strip()) # removing data in parenthesis for city so we can get lat and long\n",
    "\n",
    "# Initialize the geolocator\n",
    "geolocator = Nominatim(user_agent=\"capstone_data_pull2\")\n",
    "\n",
    "# Function to get latitude and longitude\n",
    "def get_lat_long(city, region, country):\n",
    "    location = geolocator.geocode(f\"{city}, {region}, {country}\", timeout=10)\n",
    "    if location:\n",
    "        return location.latitude, location.longitude\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "for index, row in tqdm(map_df_cited.iterrows(), total=map_df_cited.shape[0]):\n",
    "    latitude, longitude = get_lat_long(row['City'], row['Region'], row['Country'])\n",
    "    map_df_cited.at[index, 'latitude'] = latitude\n",
    "    map_df_cited.at[index, 'longitude'] = longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the column\n",
    "map_df_cited.rename(columns={'URL': 'url'}, inplace=True)\n",
    "\n",
    "# Merge the DataFrames on the 'URL' column\n",
    "map_df_cited_merged = pd.merge(map_df_cited, final_data_cited, on='url', how='inner')\n",
    "map_df_cited_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a color map for the topics\n",
    "color_map = {\n",
    "    'Community and Population Data': 'blue',\n",
    "    'Health and Demographic Surveys': 'purple',\n",
    "    'Public Records and Services': 'red',\n",
    "    'Business and Economic Indicators': 'pink',\n",
    "    'Other': 'gray'\n",
    "}\n",
    "\n",
    "\n",
    "# Map the Topic_Name to colors\n",
    "map_df_cited_merged['color'] = map_df_cited_merged['Topic_Name'].map(color_map)\n",
    "\n",
    "# Create the scatter geo plot\n",
    "fig = go.Figure(data=go.Scattergeo(\n",
    "    lon=map_df_cited_merged['longitude'],\n",
    "    lat=map_df_cited_merged['latitude'],\n",
    "    mode='markers',\n",
    "    text=map_df_cited_merged['url'],\n",
    "    hoverinfo='text',\n",
    "    marker=dict(color=map_df_cited_merged['color'], size=10, line=dict(width=1, color='black')),\n",
    "    showlegend=False\n",
    "))\n",
    "\n",
    "# Update the layout to include a legend\n",
    "fig.update_layout(\n",
    "    title='Instances of Cited Census Data',\n",
    "    geo_scope='north america',\n",
    "    legend_title_text='Topics',\n",
    "    legend=dict(\n",
    "        itemsizing='constant'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add legend items manually\n",
    "for topic, color in color_map.items():\n",
    "    fig.add_trace(go.Scattergeo(\n",
    "        lon=[None], lat=[None],\n",
    "        mode='markers',\n",
    "        marker=dict(size=10, color=color),\n",
    "        name=topic\n",
    "    ))\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed Link Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_url = final_data_repackaged['url'].tolist()\n",
    "c_url = final_data_cited['url'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repackaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing links and creating a dataframe\n",
    "from urllib.parse import urlparse # parse out urls\n",
    "\n",
    "seed_parsed_url = []\n",
    "\n",
    "for url in r_url:\n",
    "    seed_parsed = urlparse(url)\n",
    "    seed_parsed_url.append({\n",
    "        \"URL\": url,\n",
    "        \"Scheme\": seed_parsed.scheme,\n",
    "        \"Netloc\": seed_parsed.netloc,\n",
    "        \"Path\": seed_parsed.path,\n",
    "        \"Query\": seed_parsed.query\n",
    "    })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "r_seed_parsed_links = pd.DataFrame(seed_parsed_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out any row in paths that has NA or only a / in it \n",
    "r_seed_parsed_links = r_seed_parsed_links.replace('', pd.NA).dropna(subset=['Path'])\n",
    "r_seed_parsed_links = r_seed_parsed_links[r_seed_parsed_links['Path'] != '/']\n",
    "\n",
    "print(r_seed_parsed_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netloc_distribution = r_seed_parsed_links['Netloc'].value_counts()\n",
    "print(\"Netloc distribution:\")\n",
    "print(netloc_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot netloc distribution (top 10)\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_netlocs = r_seed_parsed_links['Netloc'].value_counts().nlargest(10)\n",
    "sns.barplot(x=top_netlocs.index, y=top_netlocs.values)\n",
    "plt.title('Top 10 Repackaged Seed Links Netlocs')\n",
    "plt.xlabel('Netloc')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha = 'right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing links and creating a dataframe\n",
    "from urllib.parse import urlparse # parse out urls\n",
    "\n",
    "c_seed_parsed_url = []\n",
    "\n",
    "for url in c_url:\n",
    "    c_seed_parsed = urlparse(url)\n",
    "    c_seed_parsed_url.append({\n",
    "        \"URL\": url,\n",
    "        \"Scheme\": c_seed_parsed.scheme,\n",
    "        \"Netloc\": c_seed_parsed.netloc,\n",
    "        \"Path\": c_seed_parsed.path,\n",
    "        \"Query\": c_seed_parsed.query\n",
    "    })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "c_seed_parsed_links = pd.DataFrame(c_seed_parsed_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out any row in paths that has NA or only a / in it \n",
    "c_seed_parsed_links = c_seed_parsed_links.replace('', pd.NA).dropna(subset=['Path'])\n",
    "c_seed_parsed_links = c_seed_parsed_links[c_seed_parsed_links['Path'] != '/']\n",
    "\n",
    "print(c_seed_parsed_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netloc_distribution = c_seed_parsed_links['Netloc'].value_counts()\n",
    "print(\"Netloc distribution:\")\n",
    "print(netloc_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot netloc distribution (top 10)\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_netlocs = c_seed_parsed_links['Netloc'].value_counts().nlargest(10)\n",
    "sns.barplot(x=top_netlocs.index, y=top_netlocs.values)\n",
    "plt.title('Top 10 Cited Seed Links Netlocs')\n",
    "plt.xlabel('Netloc')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha = 'right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USCB Link Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse # parse out urls\n",
    "\n",
    "#Saving Found Links into a list for later analysis\n",
    "urls = foundlinks['Found Links'].tolist()\n",
    "\n",
    "# Filter out URLs that are just 'census.gov' or 'census.gov/'\n",
    "filtered_urls = [\n",
    "    url for url in urls \n",
    "    if not (urlparse(url).netloc == 'www.census.gov' and (urlparse(url).path == '' or urlparse(url).path == '/'))\n",
    "]\n",
    "\n",
    "len(filtered_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify patterns for repackaged links\n",
    "data_tables_patterns = ['table', 'dataset', 'data', 'productview']\n",
    "repackaged_data_tables_urls = [url for url in filtered_urls if any(pattern in url for pattern in data_tables_patterns)]\n",
    "cited_data_tables_urls = [url for url in filtered_urls if not any(pattern in url for pattern in data_tables_patterns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grabbing number of links\n",
    "len(repackaged_data_tables_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grabbing number of links\n",
    "len(cited_data_tables_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repackaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing links and creating a dataframe\n",
    "\n",
    "r_parsed = []\n",
    "\n",
    "for url in repackaged_data_tables_urls:\n",
    "    parsed_url = urlparse(url)\n",
    "    r_parsed.append({\n",
    "        \"URL\": url,\n",
    "        \"Scheme\": parsed_url.scheme,\n",
    "        \"Netloc\": parsed_url.netloc,\n",
    "        \"Path\": parsed_url.path,\n",
    "        \"Query\": parsed_url.query\n",
    "    })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "r_parsed_links = pd.DataFrame(r_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out any row in paths that has NA or only a / in it \n",
    "r_parsed_links = r_parsed_links.replace('', pd.NA).dropna(subset=['Path'])\n",
    "r_parsed_links = r_parsed_links[r_parsed_links['Path'] != '/']\n",
    "\n",
    "print(r_parsed_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of URLs\n",
    "url_count = r_parsed_links['URL'].count()\n",
    "print(f\"Total number of URLs: {url_count}\")\n",
    "\n",
    "# Count the number of unique schemes\n",
    "unique_schemes = r_parsed_links['Scheme'].nunique()\n",
    "print(f\"Number of unique schemes: {unique_schemes}\")\n",
    "\n",
    "# Count the number of unique netlocs\n",
    "unique_netlocs = r_parsed_links['Netloc'].nunique()\n",
    "print(f\"Number of unique netlocs: {unique_netlocs}\")\n",
    "\n",
    "# Count the number of unique Paths\n",
    "unique_paths = r_parsed_links['Path'].nunique()\n",
    "print(f\"Number of unique Paths: {unique_paths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting additional insights\n",
    "from collections import Counter\n",
    "\n",
    "# 1. Identify the most common URL paths\n",
    "r_paths = r_parsed_links['Path']\n",
    "r_path_counts = Counter(r_paths)\n",
    "r_most_common_paths = r_path_counts.most_common(15)\n",
    "\n",
    "print(\"\\nMost common Repackaged URL paths:\")\n",
    "for path, count in r_most_common_paths:\n",
    "    print(f\"{path}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables, counts = zip(*r_most_common_paths)\n",
    "# Create a DataFrame from the tables and counts\n",
    "data = {'Census Table': tables, 'Number of References': counts}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create an interactive horizontal bar chart\n",
    "fig = px.bar(df, \n",
    "             x='Number of References', \n",
    "             y='Census Table', \n",
    "             orientation='h',  # Horizontal bars\n",
    "             title=\"Top 15 Most Referenced U.S. Census Paths in Repackaged Instances\",\n",
    "             labels={'Number of References': 'Number of References', 'Census Table': 'Census Table'},\n",
    "             color='Number of References',  # Color bars based on count\n",
    "             text='Number of References')  # Show count on hover\n",
    "\n",
    "# Customize the hover data (you can modify this to include more details)\n",
    "fig.update_traces(texttemplate='%{text}', textposition='outside')\n",
    "\n",
    "# Invert the y-axis to show the highest count at the top\n",
    "fig.update_yaxes(categoryorder='total descending')\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netloc_distribution = r_parsed_links['Netloc'].value_counts()\n",
    "print(\"Netloc distribution:\")\n",
    "print(netloc_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot netloc distribution (top 10)\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_netlocs = r_parsed_links['Netloc'].value_counts().nlargest(10)\n",
    "sns.barplot(x=top_netlocs.index, y=top_netlocs.values)\n",
    "plt.title('Top 10 Repackaged Netlocs')\n",
    "plt.xlabel('Netloc')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha = 'right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_query = r_parsed_links.replace('', pd.NA).dropna(subset=['Query'])\n",
    "print(r_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and count unique query parameters\n",
    "r_query['Query_Params'] = r_query['Query'].apply(lambda x: len(x.split('&')) if x else 0)\n",
    "query_params_distribution = r_query['Query_Params'].value_counts()\n",
    "print(\"Repackaged Query parameters distribution:\")\n",
    "print(query_params_distribution)\n",
    "\n",
    "# Plot query parameters distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=r_query, x='Query_Params', order=r_query['Query_Params'].value_counts().index)\n",
    "plt.title('Distribution of Repackaged Query Parameters Count')\n",
    "plt.xlabel('Number of Query Parameters')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting additional insights\n",
    "from collections import Counter\n",
    "\n",
    "# 1. Identify the most common URL paths\n",
    "r_query = r_query['Query']\n",
    "r_query_counts = Counter(r_query)\n",
    "r_most_common_query = r_query_counts.most_common(15)\n",
    "\n",
    "print(\"\\nMost common Repackaged URL paths:\")\n",
    "for path, count in r_most_common_query:\n",
    "    print(f\"{path}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables, counts = zip(*r_most_common_query)\n",
    "# Create a DataFrame from the tables and counts\n",
    "data = {'Census Table': tables, 'Number of References': counts}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create an interactive horizontal bar chart\n",
    "fig = px.bar(df, \n",
    "             x='Number of References', \n",
    "             y='Census Table', \n",
    "             orientation='h',  # Horizontal bars\n",
    "             title=\"Top 15 Most Referenced U.S. Census Queries in Repackaged Instances\",\n",
    "             labels={'Number of References': 'Number of References', 'Census Table': 'Census Table'},\n",
    "             color='Number of References',  # Color bars based on count\n",
    "             text='Number of References')  # Show count on hover\n",
    "\n",
    "# Customize the hover data (you can modify this to include more details)\n",
    "fig.update_traces(texttemplate='%{text}', textposition='outside')\n",
    "\n",
    "# Invert the y-axis to show the highest count at the top\n",
    "fig.update_yaxes(categoryorder='total descending')\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing links and creating a dataframe\n",
    "\n",
    "c_parsed = []\n",
    "\n",
    "for url in cited_data_tables_urls:\n",
    "    parsed_url = urlparse(url)\n",
    "    c_parsed.append({\n",
    "        \"URL\": url,\n",
    "        \"Scheme\": parsed_url.scheme,\n",
    "        \"Netloc\": parsed_url.netloc,\n",
    "        \"Path\": parsed_url.path,\n",
    "        \"Query\": parsed_url.query\n",
    "    })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "c_parsed_links = pd.DataFrame(c_parsed)\n",
    "\n",
    "print(c_parsed_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out any row in paths that has NA or only a / in it \n",
    "c_parsed_links = c_parsed_links.replace('', pd.NA).dropna(subset=['Path'])\n",
    "c_parsed_links = c_parsed_links[c_parsed_links['Path'] != '/']\n",
    "\n",
    "print(c_parsed_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of URLs\n",
    "url_count = c_parsed_links['URL'].count()\n",
    "print(f\"Total number of URLs: {url_count}\")\n",
    "\n",
    "# Count the number of unique schemes\n",
    "unique_schemes = c_parsed_links['Scheme'].nunique()\n",
    "print(f\"Number of unique schemes: {unique_schemes}\")\n",
    "\n",
    "# Count the number of unique netlocs\n",
    "unique_netlocs = c_parsed_links['Netloc'].nunique()\n",
    "print(f\"Number of unique netlocs: {unique_netlocs}\")\n",
    "\n",
    "# Count the number of unique Paths\n",
    "unique_paths = c_parsed_links['Path'].nunique()\n",
    "print(f\"Number of unique Paths: {unique_paths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Identify the most common URL paths\n",
    "c_paths = c_parsed_links['Path']\n",
    "c_path_counts = Counter(c_paths)\n",
    "c_most_common_paths = c_path_counts.most_common(15)\n",
    "\n",
    "print(\"\\nMost common Cited URL paths:\")\n",
    "for path, count in c_most_common_paths:\n",
    "    print(f\"{path}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables, counts = zip(*c_most_common_paths)\n",
    "# Create a DataFrame from the tables and counts\n",
    "data = {'Census Table': tables, 'Number of References': counts}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create an interactive horizontal bar chart\n",
    "fig = px.bar(df, \n",
    "             x='Number of References', \n",
    "             y='Census Table', \n",
    "             orientation='h',  # Horizontal bars\n",
    "             title=\"Top 15 Most Referenced U.S. Census Paths in Cited Instances\",\n",
    "             labels={'Number of References': 'Number of References', 'Census Table': 'Census Table'},\n",
    "             color='Number of References',  # Color bars based on count\n",
    "             text='Number of References')  # Show count on hover\n",
    "\n",
    "# Customize the hover data (you can modify this to include more details)\n",
    "fig.update_traces(texttemplate='%{text}', textposition='outside')\n",
    "\n",
    "# Invert the y-axis to show the highest count at the top\n",
    "fig.update_yaxes(categoryorder='total descending')\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netloc_distribution = c_parsed_links['Netloc'].value_counts()\n",
    "print(\"Netloc distribution:\")\n",
    "print(netloc_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot netloc distribution (top 10)\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_netlocs = c_parsed_links['Netloc'].value_counts().nlargest(10)\n",
    "sns.barplot(x=top_netlocs.index, y=top_netlocs.values)\n",
    "plt.title('Top 10 Cited Netlocs')\n",
    "plt.xlabel('Netloc')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_query = c_parsed_links.replace('', pd.NA).dropna(subset=['Query'])\n",
    "print(c_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and count unique query parameters\n",
    "c_query['Query_Params'] = c_query['Query'].apply(lambda x: len(x.split('&')) if x else 0)\n",
    "query_params_distribution = c_query['Query_Params'].value_counts()\n",
    "print(\"Cited Query parameters distribution:\")\n",
    "print(query_params_distribution)\n",
    "\n",
    "# Plot query parameters distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=c_query, x='Query_Params', order=c_query['Query_Params'].value_counts().index)\n",
    "plt.title('Distribution of Cited Query Parameters Count')\n",
    "plt.xlabel('Number of Query Parameters')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Identify the most common URL Queries\n",
    "c_query = c_query['Query']\n",
    "c_query_counts = Counter(c_query)\n",
    "c_most_common_query = c_query_counts.most_common(10)\n",
    "\n",
    "print(\"\\nMost common Cited URL Queries:\")\n",
    "for path, count in c_most_common_query:\n",
    "    print(f\"{path}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables, counts = zip(*c_most_common_query)\n",
    "# Create a DataFrame from the tables and counts\n",
    "data = {'Census Table': tables, 'Number of References': counts}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create an interactive horizontal bar chart\n",
    "fig = px.bar(df, \n",
    "             x='Number of References', \n",
    "             y='Census Table', \n",
    "             orientation='h',  # Horizontal bars\n",
    "             title=\"Top 15 Most Referenced Queries in Cited Instances\",\n",
    "             labels={'Number of References': 'Number of References', 'Census Table': 'Census Table'},\n",
    "             color='Number of References',  # Color bars based on count\n",
    "             text='Number of References')  # Show count on hover\n",
    "\n",
    "# Customize the hover data (you can modify this to include more details)\n",
    "fig.update_traces(texttemplate='%{text}', textposition='outside')\n",
    "\n",
    "# Invert the y-axis to show the highest count at the top\n",
    "fig.update_yaxes(categoryorder='total descending')\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
