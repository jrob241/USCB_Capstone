{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering and Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling in WET Files from Common Crawl to create our data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wet File Directory 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # http request\n",
    "import gzip # decompresses WET files\n",
    "import os # make folder/directory for WET files\n",
    "import warcio # iterates over file content\n",
    "from warcio.archiveiterator import ArchiveIterator # parses and saves content\n",
    "\n",
    "# List of URLs of WET files you want to download\n",
    "wet_file_urls = [\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198865694.2/wet/CC-MAIN-20240625072502-20240625102502-00065.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861643.92/wet/CC-MAIN-20240616043719-20240616073719-00698.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861741.14/wet/CC-MAIN-20240617215859-20240618005859-00395.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862474.84/wet/CC-MAIN-20240623131446-20240623161446-00182.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861747.46/wet/CC-MAIN-20240618042809-20240618072809-00644.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861883.41/wet/CC-MAIN-20240620043158-20240620073158-00174.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861701.67/wet/CC-MAIN-20240617091230-20240617121230-00365.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862410.56/wet/CC-MAIN-20240622175245-20240622205245-00784.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861520.44/wet/CC-MAIN-20240614012527-20240614042527-00773.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861451.34/wet/CC-MAIN-20240613123217-20240613153217-00530.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861853.72/wet/CC-MAIN-20240619220908-20240620010908-00540.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861701.67/wet/CC-MAIN-20240617091230-20240617121230-00856.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861940.83/wet/CC-MAIN-20240620105805-20240620135805-00605.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861451.34/wet/CC-MAIN-20240613123217-20240613153217-00492.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862006.96/wet/CC-MAIN-20240620204310-20240620234310-00088.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861698.15/wet/CC-MAIN-20240617060013-20240617090013-00216.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861520.44/wet/CC-MAIN-20240614012527-20240614042527-00586.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198865074.62/wet/CC-MAIN-20240624052615-20240624082615-00262.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862404.32/wet/CC-MAIN-20240622144011-20240622174011-00288.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198865401.1/wet/CC-MAIN-20240624151022-20240624181022-00847.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862032.71/wet/CC-MAIN-20240620235751-20240621025751-00595.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861741.14/wet/CC-MAIN-20240617215859-20240618005859-00133.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861701.67/wet/CC-MAIN-20240617091230-20240617121230-00596.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861173.16/wet/CC-MAIN-20240612140424-20240612170424-00154.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861372.90/wet/CC-MAIN-20240613091959-20240613121959-00290.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862070.5/wet/CC-MAIN-20240621093735-20240621123735-00091.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861575.66/wet/CC-MAIN-20240614204739-20240614234739-00073.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861733.59/wet/CC-MAIN-20240617153913-20240617183913-00516.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861671.61/wet/CC-MAIN-20240616203247-20240616233247-00103.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861883.41/wet/CC-MAIN-20240620043158-20240620073158-00088.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861940.83/wet/CC-MAIN-20240620105805-20240620135805-00105.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861657.69/wet/CC-MAIN-20240616074847-20240616104847-00885.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861916.26/wet/CC-MAIN-20240620074431-20240620104431-00059.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862157.88/wet/CC-MAIN-20240621191840-20240621221840-00658.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861261.53/wet/CC-MAIN-20240612203157-20240612233157-00115.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862466.81/wet/CC-MAIN-20240623100101-20240623130101-00616.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861372.90/wet/CC-MAIN-20240613091959-20240613121959-00561.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861747.70/wet/CC-MAIN-20240618073942-20240618103942-00482.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861797.58/wet/CC-MAIN-20240619025415-20240619055415-00303.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198866218.13/wet/CC-MAIN-20240625171218-20240625201218-00605.warc.wet.gz',\n",
    "]\n",
    "\n",
    "# Directory to save downloaded WET files\n",
    "download_dir = 'wet_files'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "# Download each WET file\n",
    "for url in wet_file_urls:\n",
    "    filename = os.path.join(download_dir, url.split('/')[-1])\n",
    "    print(f'Downloading {filename}...')\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f'{filename} downloaded successfully.')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Failed to download {filename}. Error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WET File Directory 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of URLs of WET files you want to download\n",
    "wet_file_urls = [\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862425.28/wet/CC-MAIN-20240623001858-20240623031858-00565.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861916.26/wet/CC-MAIN-20240620074431-20240620104431-00473.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861794.76/wet/CC-MAIN-20240618203026-20240618233026-00655.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861575.66/wet/CC-MAIN-20240614204739-20240614234739-00283.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862466.81/wet/CC-MAIN-20240623100101-20240623130101-00268.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198865972.21/wet/CC-MAIN-20240625104040-20240625134040-00660.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862070.5/wet/CC-MAIN-20240621093735-20240621123735-00448.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861173.16/wet/CC-MAIN-20240612140424-20240612170424-00660.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861659.47/wet/CC-MAIN-20240616105959-20240616135959-00736.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862425.28/wet/CC-MAIN-20240623001858-20240623031858-00773.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861670.48/wet/CC-MAIN-20240616172129-20240616202129-00611.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861567.95/wet/CC-MAIN-20240614141929-20240614171929-00725.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861546.27/wet/CC-MAIN-20240614110447-20240614140447-00462.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198864850.31/wet/CC-MAIN-20240623194302-20240623224302-00330.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862157.88/wet/CC-MAIN-20240621191840-20240621221840-00189.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861480.87/wet/CC-MAIN-20240613154645-20240613184645-00685.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862157.88/wet/CC-MAIN-20240621191840-20240621221840-00434.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198866218.13/wet/CC-MAIN-20240625171218-20240625201218-00428.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861659.47/wet/CC-MAIN-20240616105959-20240616135959-00858.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861594.22/wet/CC-MAIN-20240615124455-20240615154455-00398.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861940.83/wet/CC-MAIN-20240620105805-20240620135805-00107.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861746.4/wet/CC-MAIN-20240618011430-20240618041430-00048.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861880.60/wet/CC-MAIN-20240620011821-20240620041821-00008.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861883.41/wet/CC-MAIN-20240620043158-20240620073158-00617.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862132.50/wet/CC-MAIN-20240621160500-20240621190500-00463.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861796.49/wet/CC-MAIN-20240618234039-20240619024039-00133.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862425.28/wet/CC-MAIN-20240623001858-20240623031858-00114.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198865560.33/wet/CC-MAIN-20240625041023-20240625071023-00230.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862466.81/wet/CC-MAIN-20240623100101-20240623130101-00024.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861545.42/wet/CC-MAIN-20240614075213-20240614105213-00092.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198866218.13/wet/CC-MAIN-20240625171218-20240625201218-00737.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198865560.33/wet/CC-MAIN-20240625041023-20240625071023-00290.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198865401.1/wet/CC-MAIN-20240624151022-20240624181022-00584.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862132.50/wet/CC-MAIN-20240621160500-20240621190500-00364.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861741.14/wet/CC-MAIN-20240617215859-20240618005859-00426.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861762.73/wet/CC-MAIN-20240618140737-20240618170737-00579.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861606.63/wet/CC-MAIN-20240615190624-20240615220624-00502.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861578.89/wet/CC-MAIN-20240614235857-20240615025857-00178.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861806.64/wet/CC-MAIN-20240619060341-20240619090341-00180.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861883.41/wet/CC-MAIN-20240620043158-20240620073158-00395.warc.wet.gz',\n",
    "]\n",
    "\n",
    "# Directory to save downloaded WET files\n",
    "download_dir = 'wet_files2'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "# Download each WET file\n",
    "for url in wet_file_urls:\n",
    "    filename = os.path.join(download_dir, url.split('/')[-1])\n",
    "    print(f'Downloading {filename}...')\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f'{filename} downloaded successfully.')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Failed to download {filename}. Error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WET File Directory 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of URLs of WET files you want to download\n",
    "wet_file_urls = [\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861640.68/wet/CC-MAIN-20240616012706-20240616042706-00362.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861989.79/wet/CC-MAIN-20240620172726-20240620202726-00429.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198865074.62/wet/CC-MAIN-20240624052615-20240624082615-00309.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198865545.19/wet/CC-MAIN-20240625005529-20240625035529-00161.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861342.11/wet/CC-MAIN-20240613025523-20240613055523-00439.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861794.76/wet/CC-MAIN-20240618203026-20240618233026-00253.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861733.59/wet/CC-MAIN-20240617153913-20240617183913-00311.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861517.98/wet/CC-MAIN-20240613221354-20240614011354-00687.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861797.58/wet/CC-MAIN-20240619025415-20240619055415-00397.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861659.47/wet/CC-MAIN-20240616105959-20240616135959-00010.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862032.71/wet/CC-MAIN-20240620235751-20240621025751-00177.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861762.73/wet/CC-MAIN-20240618140737-20240618170737-00491.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198866143.18/wet/CC-MAIN-20240625135622-20240625165622-00814.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861546.27/wet/CC-MAIN-20240614110447-20240614140447-00439.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861342.11/wet/CC-MAIN-20240613025523-20240613055523-00039.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861451.34/wet/CC-MAIN-20240613123217-20240613153217-00591.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198864986.57/wet/CC-MAIN-20240624021134-20240624051134-00669.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198865490.6/wet/CC-MAIN-20240624214047-20240625004047-00052.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861659.47/wet/CC-MAIN-20240616105959-20240616135959-00208.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861741.14/wet/CC-MAIN-20240617215859-20240618005859-00099.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861183.54/wet/CC-MAIN-20240612171727-20240612201727-00191.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861261.53/wet/CC-MAIN-20240612203157-20240612233157-00325.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861319.37/wet/CC-MAIN-20240612234213-20240613024213-00112.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861342.11/wet/CC-MAIN-20240613025523-20240613055523-00436.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198861372.90/wet/CC-MAIN-20240613091959-20240613121959-00418.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198865545.19/wet/CC-MAIN-20240625005529-20240625035529-00351.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198865560.33/wet/CC-MAIN-20240625041023-20240625071023-00028.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198865694.2/wet/CC-MAIN-20240625072502-20240625102502-00731.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198865972.21/wet/CC-MAIN-20240625104040-20240625134040-00560.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198866218.13/wet/CC-MAIN-20240625171218-20240625201218-00522.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198866422.9/wet/CC-MAIN-20240625202802-20240625232802-00899.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198865490.6/wet/CC-MAIN-20240624214047-20240625004047-00168.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198865482.23/wet/CC-MAIN-20240624182503-20240624212503-00744.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862488.55/wet/CC-MAIN-20240623162925-20240623192925-00323.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862474.84/wet/CC-MAIN-20240623131446-20240623161446-00826.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862466.81/wet/CC-MAIN-20240623100101-20240623130101-00533.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862464.38/wet/CC-MAIN-20240623064523-20240623094523-00155.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862430.93/wet/CC-MAIN-20240623033236-20240623063236-00058.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862425.28/wet/CC-MAIN-20240623001858-20240623031858-00815.warc.wet.gz',\n",
    "'https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-26/segments/1718198862420.91/wet/CC-MAIN-20240622210521-20240623000521-00856.warc.wet.gz',\n",
    "]\n",
    "\n",
    "# Directory to save downloaded WET files\n",
    "download_dir = 'wet_files3'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "# Download each WET file\n",
    "for url in wet_file_urls:\n",
    "    filename = os.path.join(download_dir, url.split('/')[-1])\n",
    "    print(f'Downloading {filename}...')\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f'{filename} downloaded successfully.')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Failed to download {filename}. Error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing WET file directory 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # used for creating/manipulating data frames\n",
    "\n",
    "# Directory where the WET files are saved\n",
    "download_dir = 'wet_files'\n",
    "\n",
    "def process_wet_file(file_path):\n",
    "    \"\"\"Process a single WET file and return a list of dictionaries with URL, content, and language.\"\"\"\n",
    "    data = []\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        for record in ArchiveIterator(f):\n",
    "            if record.rec_type == 'conversion':\n",
    "                content = record.content_stream().read().decode('utf-8')\n",
    "                url = record.rec_headers.get_header('WARC-Target-URI')\n",
    "                language = record.rec_headers.get_header('WARC-Identified-Content-Language')\n",
    "                data.append({'url': url, 'content': content, 'language': language})\n",
    "    return data\n",
    "\n",
    "def parse_wet_files(directory):\n",
    "    \"\"\"Iterate over all WET files in the specified directory and process them.\"\"\"\n",
    "    all_data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.warc.wet.gz'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            print(f'Processing {file_path}...')\n",
    "            file_data = process_wet_file(file_path)\n",
    "            all_data.extend(file_data)\n",
    "    return all_data\n",
    "\n",
    "def main():\n",
    "    # Parse all WET files in the directory\n",
    "    data = parse_wet_files(download_dir)\n",
    "    \n",
    "    # Convert the list of dictionaries to a pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    print(\"Processing completed!\")\n",
    "    return df\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df # view data frame 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-English web pages\n",
    "df = df[df['language']=='eng']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing WET file directory 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the WET files are saved\n",
    "download_dir = 'wet_files2'\n",
    "\n",
    "def process_wet_file(file_path):\n",
    "    \"\"\"Process a single WET file and return a list of dictionaries with URL, content, and language.\"\"\"\n",
    "    data = []\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        for record in ArchiveIterator(f):\n",
    "            if record.rec_type == 'conversion':\n",
    "                content = record.content_stream().read().decode('utf-8')\n",
    "                url = record.rec_headers.get_header('WARC-Target-URI')\n",
    "                language = record.rec_headers.get_header('WARC-Identified-Content-Language')\n",
    "                data.append({'url': url, 'content': content, 'language': language})\n",
    "    return data\n",
    "\n",
    "def parse_wet_files(directory):\n",
    "    \"\"\"Iterate over all WET files in the specified directory and process them.\"\"\"\n",
    "    all_data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.warc.wet.gz'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            print(f'Processing {file_path}...')\n",
    "            file_data = process_wet_file(file_path)\n",
    "            all_data.extend(file_data)\n",
    "    return all_data\n",
    "\n",
    "def main():\n",
    "    # Parse all WET files in the directory\n",
    "    data = parse_wet_files(download_dir)\n",
    "    \n",
    "    # Convert the list of dictionaries to a pandas DataFrame\n",
    "    df2 = pd.DataFrame(data)\n",
    "\n",
    "    print(\"Processing completed!\")\n",
    "    return df2\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    df2 = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 # view data frame 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-English web pages\n",
    "df2 = df2[df2['language']=='eng']\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing WET file directory 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the WET files are saved\n",
    "download_dir = 'wet_files3'\n",
    "\n",
    "def process_wet_file(file_path):\n",
    "    \"\"\"Process a single WET file and return a list of dictionaries with URL, content, and language.\"\"\"\n",
    "    data = []\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        for record in ArchiveIterator(f):\n",
    "            if record.rec_type == 'conversion':\n",
    "                content = record.content_stream().read().decode('utf-8')\n",
    "                url = record.rec_headers.get_header('WARC-Target-URI')\n",
    "                language = record.rec_headers.get_header('WARC-Identified-Content-Language')\n",
    "                data.append({'url': url, 'content': content, 'language': language})\n",
    "    return data\n",
    "\n",
    "def parse_wet_files(directory):\n",
    "    \"\"\"Iterate over all WET files in the specified directory and process them.\"\"\"\n",
    "    all_data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.warc.wet.gz'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            print(f'Processing {file_path}...')\n",
    "            file_data = process_wet_file(file_path)\n",
    "            all_data.extend(file_data)\n",
    "    return all_data\n",
    "\n",
    "def main():\n",
    "    # Parse all WET files in the directory\n",
    "    data = parse_wet_files(download_dir)\n",
    "    \n",
    "    # Convert the list of dictionaries to a pandas DataFrame\n",
    "    df3 = pd.DataFrame(data)\n",
    "\n",
    "    print(\"Processing completed!\")\n",
    "    return df3\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    df3 = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 # view data frame 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-English web pages\n",
    "df3 = df3[df3['language']=='eng']\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the 3 data frames for further processing\n",
    "webpage_data = pd.concat([df,df2,df3])\n",
    "webpage_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop language column and seperate out URL's to run through the scraper\n",
    "webpage_data = webpage_data.drop(columns=[\"language\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Cleaning in Manual Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out the web page data into chunks of 100,000. We had to do this in order to clean the code without the kernel crashing. \n",
    "# Even using a 'for loop' did not work and continued to crash. However, I think this issue was due our limited computing power\n",
    "content1 = webpage_data[0:100000]\n",
    "content2 = webpage_data[100000:200000]\n",
    "content3 = webpage_data[200000:300000]\n",
    "content4 = webpage_data[300000:400000]\n",
    "content5 = webpage_data[400000:500000]\n",
    "content6 = webpage_data[500000:600000]\n",
    "content7 = webpage_data[600000:700000]\n",
    "content8 = webpage_data[700000:800000]\n",
    "content9 = webpage_data[800000:900000]\n",
    "content10 = webpage_data[900000:1000000]\n",
    "content11 = webpage_data[1000000:1100000]\n",
    "content12 = webpage_data[1100000:1200000]\n",
    "content13 = webpage_data[1200000:1300000]\n",
    "content14 = webpage_data[1300000:1400000]\n",
    "content15 = webpage_data[1400000:1483258]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern to match all special characters\n",
    "pattern = r'[^a-zA-Z0-9\\s]'\n",
    "\n",
    "# Replace special characters with an empty string\n",
    "content1['content'] = content1['content'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "# Remove newline characters\n",
    "content1['content'] = content1['content'].str.replace(r'\\n', '', regex=True)\n",
    "\n",
    "content1 = content1.applymap(lambda x: x.lower() if isinstance(x, str) else x) #lowercase\n",
    "\n",
    "print(content1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern to match all special characters\n",
    "pattern = r'[^a-zA-Z0-9\\s]'\n",
    "\n",
    "# Replace special characters with an empty string\n",
    "content2['content'] = content2['content'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "# Remove newline characters\n",
    "content2['content'] = content2['content'].str.replace(r'\\n', '', regex=True)\n",
    "\n",
    "content2 = content2.applymap(lambda x: x.lower() if isinstance(x, str) else x) #lowercase\n",
    "\n",
    "print(content2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern to match all special characters\n",
    "pattern = r'[^a-zA-Z0-9\\s]'\n",
    "\n",
    "# Replace special characters with an empty string\n",
    "content3['content'] = content3['content'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "# Remove newline characters\n",
    "content3['content'] = content3['content'].str.replace(r'\\n', '', regex=True)\n",
    "\n",
    "content3 = content3.applymap(lambda x: x.lower() if isinstance(x, str) else x) #lowercase\n",
    "\n",
    "print(content3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern to match all special characters\n",
    "pattern = r'[^a-zA-Z0-9\\s]'\n",
    "\n",
    "# Replace special characters with an empty string\n",
    "content4['content'] = content4['content'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "# Remove newline characters\n",
    "content4['content'] = content4['content'].str.replace(r'\\n', '', regex=True)\n",
    "\n",
    "content4 = content4.applymap(lambda x: x.lower() if isinstance(x, str) else x) #lowercase\n",
    "\n",
    "print(content4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern to match all special characters\n",
    "pattern = r'[^a-zA-Z0-9\\s]'\n",
    "\n",
    "# Replace special characters with an empty string\n",
    "content5['content'] = content5['content'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "# Remove newline characters\n",
    "content5['content'] = content5['content'].str.replace(r'\\n', '', regex=True)\n",
    "\n",
    "content5 = content5.applymap(lambda x: x.lower() if isinstance(x, str) else x) #lowercase\n",
    "\n",
    "print(content5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern to match all special characters\n",
    "pattern = r'[^a-zA-Z0-9\\s]'\n",
    "\n",
    "# Replace special characters with an empty string\n",
    "content6['content'] = content6['content'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "# Remove newline characters\n",
    "content6['content'] = content6['content'].str.replace(r'\\n', '', regex=True)\n",
    "\n",
    "content6 = content6.applymap(lambda x: x.lower() if isinstance(x, str) else x) #lowercase\n",
    "\n",
    "print(content6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern to match all special characters\n",
    "pattern = r'[^a-zA-Z0-9\\s]'\n",
    "\n",
    "# Replace special characters with an empty string\n",
    "content7['content'] = content7['content'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "# Remove newline characters\n",
    "content7['content'] = content7['content'].str.replace(r'\\n', '', regex=True)\n",
    "\n",
    "content7 = content7.applymap(lambda x: x.lower() if isinstance(x, str) else x) #lowercase\n",
    "\n",
    "print(content7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern to match all special characters\n",
    "pattern = r'[^a-zA-Z0-9\\s]'\n",
    "\n",
    "# Replace special characters with an empty string\n",
    "content8['content'] = content8['content'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "# Remove newline characters\n",
    "content8['content'] = content8['content'].str.replace(r'\\n', '', regex=True)\n",
    "\n",
    "content8 = content8.applymap(lambda x: x.lower() if isinstance(x, str) else x) #lowercase\n",
    "\n",
    "print(content8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern to match all special characters\n",
    "pattern = r'[^a-zA-Z0-9\\s]'\n",
    "\n",
    "# Replace special characters with an empty string\n",
    "content9['content'] = content9['content'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "# Remove newline characters\n",
    "content9['content'] = content9['content'].str.replace(r'\\n', '', regex=True)\n",
    "\n",
    "content9 = content9.applymap(lambda x: x.lower() if isinstance(x, str) else x) #lowercase\n",
    "\n",
    "print(content9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern to match all special characters\n",
    "pattern = r'[^a-zA-Z0-9\\s]'\n",
    "\n",
    "# Replace special characters with an empty string\n",
    "content10['content'] = content10['content'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "# Remove newline characters\n",
    "content10['content'] = content10['content'].str.replace(r'\\n', '', regex=True)\n",
    "\n",
    "content10 = content10.applymap(lambda x: x.lower() if isinstance(x, str) else x) #lowercase\n",
    "\n",
    "print(content10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern to match all special characters\n",
    "pattern = r'[^a-zA-Z0-9\\s]'\n",
    "\n",
    "# Replace special characters with an empty string\n",
    "content11['content'] = content11['content'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "# Remove newline characters\n",
    "content11['content'] = content11['content'].str.replace(r'\\n', '', regex=True)\n",
    "\n",
    "content11 = content11.applymap(lambda x: x.lower() if isinstance(x, str) else x) #lowercase\n",
    "\n",
    "print(content11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern to match all special characters\n",
    "pattern = r'[^a-zA-Z0-9\\s]'\n",
    "\n",
    "# Replace special characters with an empty string\n",
    "content12['content'] = content12['content'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "# Remove newline characters\n",
    "content12['content'] = content12['content'].str.replace(r'\\n', '', regex=True)\n",
    "\n",
    "content12 = content12.applymap(lambda x: x.lower() if isinstance(x, str) else x) #lowercase\n",
    "\n",
    "print(content12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern to match all special characters\n",
    "pattern = r'[^a-zA-Z0-9\\s]'\n",
    "\n",
    "# Replace special characters with an empty string\n",
    "content13['content'] = content13['content'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "# Remove newline characters\n",
    "content13['content'] = content13['content'].str.replace(r'\\n', '', regex=True)\n",
    "\n",
    "content13 = content13.applymap(lambda x: x.lower() if isinstance(x, str) else x) #lowercase\n",
    "\n",
    "print(content13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern to match all special characters\n",
    "pattern = r'[^a-zA-Z0-9\\s]'\n",
    "\n",
    "# Replace special characters with an empty string\n",
    "content14['content'] = content14['content'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "# Remove newline characters\n",
    "content14['content'] = content14['content'].str.replace(r'\\n', '', regex=True)\n",
    "\n",
    "content14 = content14.applymap(lambda x: x.lower() if isinstance(x, str) else x) #lowercase\n",
    "\n",
    "print(content14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern to match all special characters\n",
    "pattern = r'[^a-zA-Z0-9\\s]'\n",
    "\n",
    "# Replace special characters with an empty string\n",
    "content15['content'] = content15['content'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "# Remove newline characters\n",
    "content15['content'] = content15['content'].str.replace(r'\\n', '', regex=True)\n",
    "\n",
    "content15 = content15.applymap(lambda x: x.lower() if isinstance(x, str) else x) #lowercase\n",
    "\n",
    "print(content15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recombine cleaned content into a consolidated data frame\n",
    "\n",
    "webpage_data_clean = pd.concat([content1,content2,content3,content4,content5,content6,content7,\n",
    "                content8,content9,content10,content11,content12,content13,\n",
    "                content14,content15])\n",
    "webpage_data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Word Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We used a range of keywords to narrow down the very large amount of data\n",
    "\n",
    "# key words\n",
    "keywords = ['census', 'uscb', 'american community survey','american','nih','government','survey','population']\n",
    "\n",
    "# Function to check if any keywords are in the content\n",
    "def contains_keywords(content, keywords):\n",
    "    return any(keyword in content for keyword in keywords)\n",
    "\n",
    "# Apply the function to create a new column\n",
    "webpage_data_clean['contains_keywords'] = webpage_data_clean['content'].apply(lambda x: contains_keywords(x, keywords))\n",
    "\n",
    "webpage_data_clean # view data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# narrow data to only those that contain keywords\n",
    "\n",
    "keywords_present_df = webpage_data_clean[webpage_data_clean['contains_keywords']==1]\n",
    "keywords_present_df # view data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the keyword indicator column\n",
    "\n",
    "keywords_present_df = keywords_present_df.drop(columns=[\"contains_keywords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_present_df # view the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the URLs needed to be scraped\n",
    "\n",
    "seedlinks = keywords_present_df['url'].tolist()\n",
    "\n",
    "len(seedlinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # parses HTML content\n",
    "import re # allows us to search for repackaged Census data\n",
    "\n",
    "\n",
    "# List of URLs to process\n",
    "\n",
    "urls = seedlinks[0:20000]\n",
    "\n",
    "\n",
    "# Compile the regex pattern\n",
    "pattern = re.compile(r'.census.gov.*') # will search for references to the Census website\n",
    "\n",
    "# Initialize an empty list to store all data\n",
    "all_data = []\n",
    "\n",
    "# Iterate over each URL in the list\n",
    "for url in urls:\n",
    "    try:\n",
    "        # Attempt to fetch the content from the URL\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        \n",
    "        # Parse the content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all links matching the pattern\n",
    "        links = [link.get('href') for link in soup.find_all('a', href=pattern)]\n",
    "\n",
    "        if links:\n",
    "            all_data.extend([(url, link) for link in links])\n",
    "        else:\n",
    "            all_data.append((url, 'NONE'))\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle any exception raised during the request\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        # Add a failure entry to the data\n",
    "        all_data.append((url, 'ERROR'))\n",
    "\n",
    "# Create a DataFrame with the seed links and the found links\n",
    "scrapedlinks = pd.DataFrame(all_data, columns=['Seed Link', 'Found Links'])\n",
    "print(scrapedlinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of URLs to process\n",
    "\n",
    "urls = seedlinks[20000:40000]\n",
    "\n",
    "\n",
    "# Compile the regex pattern\n",
    "pattern = re.compile(r'.census.gov.*') # will search for references to the Census website\n",
    "\n",
    "# Initialize an empty list to store all data\n",
    "all_data = []\n",
    "\n",
    "# Iterate over each URL in the list\n",
    "for url in urls:\n",
    "    try:\n",
    "        # Attempt to fetch the content from the URL\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        \n",
    "        # Parse the content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all links matching the pattern\n",
    "        links = [link.get('href') for link in soup.find_all('a', href=pattern)]\n",
    "\n",
    "        if links:\n",
    "            all_data.extend([(url, link) for link in links])\n",
    "        else:\n",
    "            all_data.append((url, 'NONE'))\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle any exception raised during the request\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        # Add a failure entry to the data\n",
    "        all_data.append((url, 'ERROR'))\n",
    "\n",
    "# Create a DataFrame with the seed links and the found links\n",
    "scrapedlinks2 = pd.DataFrame(all_data, columns=['Seed Link', 'Found Links'])\n",
    "print(scrapedlinks2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of URLs to process\n",
    "\n",
    "urls = seedlinks[40000:60000]\n",
    "\n",
    "\n",
    "# Compile the regex pattern\n",
    "pattern = re.compile(r'.census.gov.*') # will search for references to the Census website\n",
    "\n",
    "# Initialize an empty list to store all data\n",
    "all_data = []\n",
    "\n",
    "# Iterate over each URL in the list\n",
    "for url in urls:\n",
    "    try:\n",
    "        # Attempt to fetch the content from the URL\n",
    "        response = requests.get(url, timeout=4)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        \n",
    "        # Parse the content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all links matching the pattern\n",
    "        links = [link.get('href') for link in soup.find_all('a', href=pattern)]\n",
    "\n",
    "        if links:\n",
    "            all_data.extend([(url, link) for link in links])\n",
    "        else:\n",
    "            all_data.append((url, 'NONE'))\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle any exception raised during the request\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        # Add a failure entry to the data\n",
    "        all_data.append((url, 'ERROR'))\n",
    "\n",
    "# Create a DataFrame with the seed links and the found links\n",
    "scrapedlinks3 = pd.DataFrame(all_data, columns=['Seed Link', 'Found Links'])\n",
    "print(scrapedlinks3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of URLs to process\n",
    "\n",
    "urls = seedlinks[60000:80000]\n",
    "\n",
    "\n",
    "# Compile the regex pattern\n",
    "pattern = re.compile(r'.census.gov.*') # will search for references to the Census website\n",
    "\n",
    "# Initialize an empty list to store all data\n",
    "all_data = []\n",
    "\n",
    "# Iterate over each URL in the list\n",
    "for url in urls:\n",
    "    try:\n",
    "        # Attempt to fetch the content from the URL\n",
    "        response = requests.get(url, timeout=4)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        \n",
    "        # Parse the content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all links matching the pattern\n",
    "        links = [link.get('href') for link in soup.find_all('a', href=pattern)]\n",
    "\n",
    "        if links:\n",
    "            all_data.extend([(url, link) for link in links])\n",
    "        else:\n",
    "            all_data.append((url, 'NONE'))\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle any exception raised during the request\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        # Add a failure entry to the data\n",
    "        all_data.append((url, 'ERROR'))\n",
    "\n",
    "# Create a DataFrame with the seed links and the found links\n",
    "scrapedlinks4 = pd.DataFrame(all_data, columns=['Seed Link', 'Found Links'])\n",
    "print(scrapedlinks4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of URLs to process\n",
    "\n",
    "urls = seedlinks[80000:100000]\n",
    "\n",
    "\n",
    "# Compile the regex pattern\n",
    "pattern = re.compile(r'.census.gov.*') # will search for references to the Census website\n",
    "\n",
    "# Initialize an empty list to store all data\n",
    "all_data = []\n",
    "\n",
    "# Iterate over each URL in the list\n",
    "for url in urls:\n",
    "    try:\n",
    "        # Attempt to fetch the content from the URL\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        \n",
    "        # Parse the content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all links matching the pattern\n",
    "        links = [link.get('href') for link in soup.find_all('a', href=pattern)]\n",
    "\n",
    "        if links:\n",
    "            all_data.extend([(url, link) for link in links])\n",
    "        else:\n",
    "            all_data.append((url, 'NONE'))\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle any exception raised during the request\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        # Add a failure entry to the data\n",
    "        all_data.append((url, 'ERROR'))\n",
    "\n",
    "# Create a DataFrame with the seed links and the found links\n",
    "scrapedlinks5 = pd.DataFrame(all_data, columns=['Seed Link', 'Found Links'])\n",
    "print(scrapedlinks5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of URLs to process\n",
    "\n",
    "urls = seedlinks[100000:150000]\n",
    "\n",
    "\n",
    "# Compile the regex pattern\n",
    "pattern = re.compile(r'.census.gov.*') # will search for references to the Census website\n",
    "\n",
    "# Initialize an empty list to store all data\n",
    "all_data = []\n",
    "\n",
    "# Iterate over each URL in the list\n",
    "for url in urls:\n",
    "    try:\n",
    "        # Attempt to fetch the content from the URL\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        \n",
    "        # Parse the content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all links matching the pattern\n",
    "        links = [link.get('href') for link in soup.find_all('a', href=pattern)]\n",
    "\n",
    "        if links:\n",
    "            all_data.extend([(url, link) for link in links])\n",
    "        else:\n",
    "            all_data.append((url, 'NONE'))\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle any exception raised during the request\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        # Add a failure entry to the data\n",
    "        all_data.append((url, 'ERROR'))\n",
    "\n",
    "# Create a DataFrame with the seed links and the found links\n",
    "scrapedlinks6 = pd.DataFrame(all_data, columns=['Seed Link', 'Found Links'])\n",
    "print(scrapedlinks6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of URLs to process\n",
    "\n",
    "urls = seedlinks[150000:200000]\n",
    "\n",
    "\n",
    "# Compile the regex pattern\n",
    "pattern = re.compile(r'.census.gov.*') # will search for references to the Census website\n",
    "\n",
    "# Initialize an empty list to store all data\n",
    "all_data = []\n",
    "\n",
    "# Iterate over each URL in the list\n",
    "for url in urls:\n",
    "    try:\n",
    "        # Attempt to fetch the content from the URL\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        \n",
    "        # Parse the content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all links matching the pattern\n",
    "        links = [link.get('href') for link in soup.find_all('a', href=pattern)]\n",
    "\n",
    "        if links:\n",
    "            all_data.extend([(url, link) for link in links])\n",
    "        else:\n",
    "            all_data.append((url, 'NONE'))\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle any exception raised during the request\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        # Add a failure entry to the data\n",
    "        all_data.append((url, 'ERROR'))\n",
    "\n",
    "# Create a DataFrame with the seed links and the found links\n",
    "scrapedlinks7 = pd.DataFrame(all_data, columns=['Seed Link', 'Found Links'])\n",
    "print(scrapedlinks7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of URLs to process\n",
    "\n",
    "urls = seedlinks[200000:250000]\n",
    "\n",
    "\n",
    "# Compile the regex pattern\n",
    "pattern = re.compile(r'.census.gov.*') # will search for references to the Census website\n",
    "\n",
    "# Initialize an empty list to store all data\n",
    "all_data = []\n",
    "\n",
    "# Iterate over each URL in the list\n",
    "for url in urls:\n",
    "    try:\n",
    "        # Attempt to fetch the content from the URL\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        \n",
    "        # Parse the content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all links matching the pattern\n",
    "        links = [link.get('href') for link in soup.find_all('a', href=pattern)]\n",
    "\n",
    "        if links:\n",
    "            all_data.extend([(url, link) for link in links])\n",
    "        else:\n",
    "            all_data.append((url, 'NONE'))\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle any exception raised during the request\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        # Add a failure entry to the data\n",
    "        all_data.append((url, 'ERROR'))\n",
    "\n",
    "# Create a DataFrame with the seed links and the found links\n",
    "scrapedlinks8 = pd.DataFrame(all_data, columns=['Seed Link', 'Found Links'])\n",
    "print(scrapedlinks8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of URLs to process\n",
    "\n",
    "urls = seedlinks[300000:319213]\n",
    "\n",
    "\n",
    "# Compile the regex pattern\n",
    "pattern = re.compile(r'.census.gov.*') # will search for references to the Census website\n",
    "\n",
    "# Initialize an empty list to store all data\n",
    "all_data = []\n",
    "\n",
    "# Iterate over each URL in the list\n",
    "for url in urls:\n",
    "    try:\n",
    "        # Attempt to fetch the content from the URL\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        \n",
    "        # Parse the content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all links matching the pattern\n",
    "        links = [link.get('href') for link in soup.find_all('a', href=pattern)]\n",
    "\n",
    "        if links:\n",
    "            all_data.extend([(url, link) for link in links])\n",
    "        else:\n",
    "            all_data.append((url, 'NONE'))\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle any exception raised during the request\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        # Add a failure entry to the data\n",
    "        all_data.append((url, 'ERROR'))\n",
    "\n",
    "# Create a DataFrame with the seed links and the found links\n",
    "scrapedlinks9 = pd.DataFrame(all_data, columns=['Seed Link', 'Found Links'])\n",
    "print(scrapedlinks9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine final scraped data\n",
    "final_data = pd.concat([scrapedlinks, scrapedlinks2, scrapedlinks3,\n",
    "           scrapedlinks4, scrapedlinks5, scrapedlinks6,\n",
    "           scrapedlinks7, scrapedlinks8, scrapedlinks9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add back in content\n",
    "\n",
    "final_data = pd.merge(final_data, keywords_present_df, on='url', how = 'left')\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate out errored instances from the webscraper and save them for future analysis\n",
    "\n",
    "final_data_errored = final_data[final_data['Found Links']=='ERROR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate out cited/repackaged instances from the webscraper and save them for future analysis\n",
    "\n",
    "final_data_links = final_data[~final_data['Found Links'].isin(['ERROR', 'NONE'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_errored.to_excel(\"erroredlinks.modeling.xlsx\", index=False) # save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_links.to_excel(\"foundlinks.modeling.xlsx\", index=False) # save data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
